
R version 3.3.1 (2016-06-21) -- "Bug in Your Hair"
Copyright (C) 2016 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> require(glmnet)
Loading required package: glmnet
Loading required package: Matrix
Loading required package: foreach
Loaded glmnet 2.0-5

> require(caret)
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
> 
> ### Put directory of file location here
> dir <- '/home/egoren/'
> 
> ## ----data----------------------------------------------------------------
> d <- read.csv(paste0(dir, "X3.csv"))
> options(na.action = 'na.pass')
> 
> ## ----mod------------------------------------------------------------
> X <- model.matrix(SalePrice ~ (.)^2 + 0, data = d)
> train.idx <- !is.na(d$SalePrice)
> set.seed(602)
> fit <- train(x = X[train.idx,],
+              y = d$SalePrice[train.idx],
+              method = "glmnet",
+              standardize.response = TRUE, standardize = TRUE,
+              #preProcess = c("zv", "medianImpute"),
+              trControl = trainControl(method = 'repeatedcv',
+                                       number = 10, repeats = 10),
+              tuneGrid = expand.grid(.alpha = seq(0, 1, by = 0.1),
+                                     .lambda = seq(0.001, 1, by = 0.05)))
Warning message:
In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
> fit$bestTune
   alpha lambda
22   0.1  0.051
> fit$results
    alpha lambda      RMSE  Rsquared     RMSESD RsquaredSD
1     0.0  0.001 0.1235585 0.9042290 0.01482707 0.02371417
2     0.0  0.051 0.1235585 0.9042290 0.01482707 0.02371417
3     0.0  0.101 0.1235585 0.9042290 0.01482707 0.02371417
4     0.0  0.151 0.1235585 0.9042290 0.01482707 0.02371417
5     0.0  0.201 0.1235585 0.9042290 0.01482707 0.02371417
6     0.0  0.251 0.1235585 0.9042290 0.01482707 0.02371417
7     0.0  0.301 0.1235585 0.9042290 0.01482707 0.02371417
8     0.0  0.351 0.1235585 0.9042290 0.01482707 0.02371417
9     0.0  0.401 0.1235585 0.9042290 0.01482707 0.02371417
10    0.0  0.451 0.1235585 0.9042290 0.01482707 0.02371417
11    0.0  0.501 0.1235585 0.9042290 0.01482707 0.02371417
12    0.0  0.551 0.1235585 0.9042290 0.01482707 0.02371417
13    0.0  0.601 0.1235585 0.9042290 0.01482707 0.02371417
14    0.0  0.651 0.1235585 0.9042290 0.01482707 0.02371417
15    0.0  0.701 0.1235585 0.9042290 0.01482707 0.02371417
16    0.0  0.751 0.1235585 0.9042290 0.01482707 0.02371417
17    0.0  0.801 0.1235585 0.9042290 0.01482707 0.02371417
18    0.0  0.851 0.1235585 0.9042290 0.01482707 0.02371417
19    0.0  0.901 0.1235585 0.9042290 0.01482707 0.02371417
20    0.0  0.951 0.1235585 0.9042290 0.01482707 0.02371417
21    0.1  0.001 0.1264941 0.8992427 0.01707356 0.02743811
22    0.1  0.051 0.1234942 0.9042320 0.01652583 0.02603973
23    0.1  0.101 0.1240322 0.9052694 0.01521796 0.02447403
24    0.1  0.151 0.1287118 0.9005724 0.01477694 0.02465841
25    0.1  0.201 0.1342938 0.8946732 0.01464786 0.02515117
26    0.1  0.251 0.1403490 0.8879952 0.01469867 0.02578184
27    0.1  0.301 0.1465214 0.8809184 0.01477888 0.02636514
28    0.1  0.351 0.1520639 0.8750089 0.01485641 0.02670239
29    0.1  0.401 0.1568163 0.8709909 0.01494883 0.02692354
30    0.1  0.451 0.1614628 0.8675652 0.01507958 0.02714308
31    0.1  0.501 0.1661955 0.8642099 0.01524910 0.02735078
32    0.1  0.551 0.1710518 0.8607612 0.01545539 0.02759792
33    0.1  0.601 0.1760482 0.8571189 0.01563184 0.02783857
34    0.1  0.651 0.1810414 0.8536133 0.01575638 0.02809332
35    0.1  0.701 0.1858647 0.8507716 0.01587031 0.02828364
36    0.1  0.751 0.1906294 0.8483585 0.01599729 0.02845102
37    0.1  0.801 0.1954441 0.8460628 0.01613823 0.02859461
38    0.1  0.851 0.2002903 0.8438819 0.01628405 0.02872598
39    0.1  0.901 0.2051786 0.8417594 0.01644145 0.02886498
40    0.1  0.951 0.2101542 0.8395150 0.01660953 0.02901589
41    0.2  0.001 0.1278119 0.8971206 0.01775483 0.02883632
42    0.2  0.051 0.1241904 0.9047565 0.01543487 0.02490774
43    0.2  0.101 0.1337642 0.8947646 0.01487832 0.02577131
44    0.2  0.151 0.1455718 0.8811592 0.01479847 0.02673137
45    0.2  0.201 0.1552750 0.8716682 0.01486168 0.02725804
46    0.2  0.251 0.1642468 0.8648273 0.01513429 0.02778779
47    0.2  0.301 0.1739440 0.8569306 0.01539353 0.02846105
48    0.2  0.351 0.1831922 0.8508286 0.01554313 0.02888289
49    0.2  0.401 0.1922262 0.8464266 0.01577691 0.02920958
50    0.2  0.451 0.2017358 0.8413643 0.01604989 0.02953817
51    0.2  0.501 0.2116013 0.8355380 0.01636260 0.02980888
52    0.2  0.551 0.2216132 0.8293856 0.01669098 0.02985632
53    0.2  0.601 0.2316012 0.8234153 0.01705357 0.02991135
54    0.2  0.651 0.2416239 0.8172602 0.01747111 0.03006231
55    0.2  0.701 0.2517431 0.8104292 0.01791086 0.03037562
56    0.2  0.751 0.2618796 0.8028923 0.01832757 0.03104637
57    0.2  0.801 0.2717657 0.7962060 0.01875063 0.03206650
58    0.2  0.851 0.2814879 0.7900862 0.01918785 0.03316906
59    0.2  0.901 0.2911277 0.7835890 0.01960355 0.03433471
60    0.2  0.951 0.3006411 0.7767734 0.02002756 0.03554362
61    0.3  0.001 0.1283963 0.8961891 0.01802677 0.02938585
62    0.3  0.051 0.1283450 0.9004196 0.01511161 0.02541951
63    0.3  0.101 0.1452142 0.8811223 0.01484263 0.02691960
64    0.3  0.151 0.1591145 0.8681629 0.01501495 0.02783901
65    0.3  0.201 0.1733736 0.8558455 0.01537471 0.02900178
66    0.3  0.251 0.1865323 0.8479796 0.01558118 0.02956717
67    0.3  0.301 0.2004541 0.8398328 0.01593387 0.03008030
68    0.3  0.351 0.2150367 0.8304078 0.01635295 0.03015937
69    0.3  0.401 0.2296880 0.8211234 0.01689826 0.03024390
70    0.3  0.451 0.2446279 0.8104232 0.01750707 0.03074108
71    0.3  0.501 0.2595488 0.7987702 0.01809162 0.03202014
72    0.3  0.551 0.2739371 0.7895205 0.01871653 0.03368118
73    0.3  0.601 0.2881628 0.7802160 0.01935259 0.03535764
74    0.3  0.651 0.3024118 0.7687292 0.02002591 0.03724156
75    0.3  0.701 0.3164008 0.7558240 0.02071848 0.03884237
76    0.3  0.751 0.3301414 0.7403052 0.02142030 0.04065226
77    0.3  0.801 0.3433862 0.7228522 0.02203051 0.04176622
78    0.3  0.851 0.3560747 0.7013975 0.02267475 0.04291745
79    0.3  0.901 0.3677737 0.6749025 0.02308991 0.04402380
80    0.3  0.951 0.3776110 0.6691717 0.02334990 0.04571392
81    0.4  0.001 0.1287359 0.8956462 0.01816226 0.02967130
82    0.4  0.051 0.1334531 0.8947451 0.01508092 0.02622456
83    0.4  0.101 0.1544512 0.8715753 0.01493142 0.02773368
84    0.4  0.151 0.1729612 0.8554212 0.01539884 0.02934287
85    0.4  0.201 0.1904578 0.8446989 0.01568342 0.03014489
86    0.4  0.251 0.2094612 0.8317880 0.01614396 0.03048337
87    0.4  0.301 0.2286507 0.8190148 0.01681988 0.03063315
88    0.4  0.351 0.2484196 0.8032099 0.01758011 0.03165273
89    0.4  0.401 0.2674673 0.7901795 0.01835268 0.03373442
90    0.4  0.451 0.2863246 0.7773364 0.01922500 0.03609920
91    0.4  0.501 0.3052310 0.7602616 0.02013825 0.03854296
92    0.4  0.551 0.3237644 0.7390520 0.02112322 0.04092063
93    0.4  0.601 0.3413645 0.7157236 0.02194114 0.04255859
94    0.4  0.651 0.3579571 0.6833768 0.02269010 0.04524928
95    0.4  0.701 0.3721187 0.6690363 0.02318278 0.04600135
96    0.4  0.751 0.3848001 0.6687726 0.02365714 0.04679442
97    0.4  0.801 0.3957726 0.6683491 0.02410939 0.04695581
98    0.4  0.851 0.3987151       NaN 0.02292270         NA
99    0.4  0.901 0.3987151       NaN 0.02292270         NA
100   0.4  0.951 0.3987151       NaN 0.02292270         NA
101   0.5  0.001 0.1289064 0.8953740 0.01820149 0.02976193
102   0.5  0.051 0.1393298 0.8876116 0.01500767 0.02691828
103   0.5  0.101 0.1630885 0.8639401 0.01525143 0.02884966
104   0.5  0.151 0.1854006 0.8473464 0.01560924 0.03028134
105   0.5  0.201 0.2087840 0.8311582 0.01612577 0.03071623
106   0.5  0.251 0.2326792 0.8141816 0.01694976 0.03111199
107   0.5  0.301 0.2569187 0.7946655 0.01785552 0.03324769
108   0.5  0.351 0.2802454 0.7789024 0.01892789 0.03614543
109   0.5  0.401 0.3037748 0.7565787 0.02007255 0.03936282
110   0.5  0.451 0.3267599 0.7264563 0.02123257 0.04222482
111   0.5  0.501 0.3480769 0.6933700 0.02229620 0.04436613
112   0.5  0.551 0.3668141 0.6689251 0.02295566 0.04626855
113   0.5  0.601 0.3834857 0.6686468 0.02366163 0.04691230
114   0.5  0.651 0.3978190 0.6445648 0.02378399 0.03708004
115   0.5  0.701 0.3987151       NaN 0.02292270         NA
116   0.5  0.751 0.3987151       NaN 0.02292270         NA
117   0.5  0.801 0.3987151       NaN 0.02292270         NA
118   0.5  0.851 0.3987151       NaN 0.02292270         NA
119   0.5  0.901 0.3987151       NaN 0.02292270         NA
120   0.5  0.951 0.3987151       NaN 0.02292270         NA
121   0.6  0.001 0.1290255 0.8951839 0.01825564 0.02986779
122   0.6  0.051 0.1450659 0.8803509 0.01495418 0.02750823
123   0.6  0.101 0.1721509 0.8553457 0.01538706 0.02986289
124   0.6  0.151 0.1987787 0.8373647 0.01587352 0.03087867
125   0.6  0.201 0.2269764 0.8173256 0.01672730 0.03111605
126   0.6  0.251 0.2557646 0.7939673 0.01780744 0.03359675
127   0.6  0.301 0.2838548 0.7718410 0.01908879 0.03769003
128   0.6  0.351 0.3119130 0.7396126 0.02054349 0.04190014
129   0.6  0.401 0.3380785 0.7036368 0.02178467 0.04411393
130   0.6  0.451 0.3616474 0.6688494 0.02272692 0.04655650
131   0.6  0.501 0.3822120 0.6688716 0.02371802 0.04703821
132   0.6  0.551 0.3986785 0.5915891 0.02300719 0.03145574
133   0.6  0.601 0.3987151       NaN 0.02292270         NA
134   0.6  0.651 0.3987151       NaN 0.02292270         NA
135   0.6  0.701 0.3987151       NaN 0.02292270         NA
136   0.6  0.751 0.3987151       NaN 0.02292270         NA
137   0.6  0.801 0.3987151       NaN 0.02292270         NA
138   0.6  0.851 0.3987151       NaN 0.02292270         NA
139   0.6  0.901 0.3987151       NaN 0.02292270         NA
140   0.6  0.951 0.3987151       NaN 0.02292270         NA
141   0.7  0.001 0.1290886 0.8950836 0.01827711 0.02989044
142   0.7  0.051 0.1499903 0.8745869 0.01494712 0.02798416
143   0.7  0.101 0.1802725 0.8495973 0.01546731 0.03058298
144   0.7  0.151 0.2121069 0.8272671 0.01616388 0.03098475
145   0.7  0.201 0.2457057 0.7989840 0.01734239 0.03282679
146   0.7  0.251 0.2781555 0.7737331 0.01879454 0.03772059
147   0.7  0.301 0.3107796 0.7344170 0.02043469 0.04260773
148   0.7  0.351 0.3409543 0.6894059 0.02190037 0.04582132
149   0.7  0.401 0.3671279 0.6687802 0.02301607 0.04705180
150   0.7  0.451 0.3914505 0.6689114 0.02435588 0.04704651
151   0.7  0.501 0.3987151       NaN 0.02292270         NA
152   0.7  0.551 0.3987151       NaN 0.02292270         NA
153   0.7  0.601 0.3987151       NaN 0.02292270         NA
154   0.7  0.651 0.3987151       NaN 0.02292270         NA
155   0.7  0.701 0.3987151       NaN 0.02292270         NA
156   0.7  0.751 0.3987151       NaN 0.02292270         NA
157   0.7  0.801 0.3987151       NaN 0.02292270         NA
158   0.7  0.851 0.3987151       NaN 0.02292270         NA
159   0.7  0.901 0.3987151       NaN 0.02292270         NA
160   0.7  0.951 0.3987151       NaN 0.02292270         NA
161   0.8  0.001 0.1291776 0.8949395 0.01830144 0.02993212
162   0.8  0.051 0.1541934 0.8704607 0.01501826 0.02848867
163   0.8  0.101 0.1889138 0.8426569 0.01552309 0.03102548
164   0.8  0.151 0.2258077 0.8150014 0.01650697 0.03148207
165   0.8  0.201 0.2630654 0.7849693 0.01800008 0.03608297
166   0.8  0.251 0.3004191 0.7432938 0.01982421 0.04264893
167   0.8  0.301 0.3353081 0.6928267 0.02159024 0.04576624
168   0.8  0.351 0.3654695 0.6689094 0.02299373 0.04704775
169   0.8  0.401 0.3947553 0.6668507 0.02465014 0.04546541
170   0.8  0.451 0.3987151       NaN 0.02292270         NA
171   0.8  0.501 0.3987151       NaN 0.02292270         NA
172   0.8  0.551 0.3987151       NaN 0.02292270         NA
173   0.8  0.601 0.3987151       NaN 0.02292270         NA
174   0.8  0.651 0.3987151       NaN 0.02292270         NA
175   0.8  0.701 0.3987151       NaN 0.02292270         NA
176   0.8  0.751 0.3987151       NaN 0.02292270         NA
177   0.8  0.801 0.3987151       NaN 0.02292270         NA
178   0.8  0.851 0.3987151       NaN 0.02292270         NA
179   0.8  0.901 0.3987151       NaN 0.02292270         NA
180   0.8  0.951 0.3987151       NaN 0.02292270         NA
181   0.9  0.001 0.1293016 0.8947315 0.01839567 0.03012462
182   0.9  0.051 0.1585018 0.8661575 0.01515892 0.02912929
183   0.9  0.101 0.1976770 0.8347141 0.01554745 0.03098006
184   0.9  0.151 0.2392893 0.8000287 0.01679059 0.03307085
185   0.9  0.201 0.2806409 0.7618431 0.01868041 0.04032530
186   0.9  0.251 0.3207920 0.7091968 0.02077871 0.04492867
187   0.9  0.301 0.3565645 0.6689126 0.02255071 0.04704719
188   0.9  0.351 0.3907379 0.6689114 0.02453599 0.04704651
189   0.9  0.401 0.3987151       NaN 0.02292270         NA
190   0.9  0.451 0.3987151       NaN 0.02292270         NA
191   0.9  0.501 0.3987151       NaN 0.02292270         NA
192   0.9  0.551 0.3987151       NaN 0.02292270         NA
193   0.9  0.601 0.3987151       NaN 0.02292270         NA
194   0.9  0.651 0.3987151       NaN 0.02292270         NA
195   0.9  0.701 0.3987151       NaN 0.02292270         NA
196   0.9  0.751 0.3987151       NaN 0.02292270         NA
197   0.9  0.801 0.3987151       NaN 0.02292270         NA
198   0.9  0.851 0.3987151       NaN 0.02292270         NA
199   0.9  0.901 0.3987151       NaN 0.02292270         NA
200   0.9  0.951 0.3987151       NaN 0.02292270         NA
201   1.0  0.001 0.1293119 0.8947213 0.01832755 0.02997807
202   1.0  0.051 0.1630580 0.8612052 0.01531298 0.02994434
203   1.0  0.101 0.2063656 0.8262628 0.01572026 0.03119489
204   1.0  0.151 0.2521320 0.7860416 0.01732108 0.03633691
205   1.0  0.201 0.2980453 0.7299521 0.01954880 0.04380994
206   1.0  0.251 0.3401510 0.6691771 0.02164655 0.04717341
207   1.0  0.301 0.3784889 0.6689114 0.02390226 0.04704651
208   1.0  0.351 0.3987151       NaN 0.02292270         NA
209   1.0  0.401 0.3987151       NaN 0.02292270         NA
210   1.0  0.451 0.3987151       NaN 0.02292270         NA
211   1.0  0.501 0.3987151       NaN 0.02292270         NA
212   1.0  0.551 0.3987151       NaN 0.02292270         NA
213   1.0  0.601 0.3987151       NaN 0.02292270         NA
214   1.0  0.651 0.3987151       NaN 0.02292270         NA
215   1.0  0.701 0.3987151       NaN 0.02292270         NA
216   1.0  0.751 0.3987151       NaN 0.02292270         NA
217   1.0  0.801 0.3987151       NaN 0.02292270         NA
218   1.0  0.851 0.3987151       NaN 0.02292270         NA
219   1.0  0.901 0.3987151       NaN 0.02292270         NA
220   1.0  0.951 0.3987151       NaN 0.02292270         NA
> hat.train <- predict(fit$finalModel, X[train.idx,])
> hat.test <- predict(fit$finalModel, X[!train.idx,])
> 
> fit2 <- glmnet(x = X[train.idx,],
+                y = d$SalePrice[train.idx],
+                standardize.response = TRUE, standardize = TRUE,
+                lambda = fit$bestTune$lambda,
+                alpha = fit$bestTune$alpha)
> 
> save.image('/home/egoren/el3.Rdat')
> 
> 
> proc.time()
    user   system  elapsed 
3971.254  121.034 4094.296 
