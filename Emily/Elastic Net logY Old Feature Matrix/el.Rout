
R version 3.3.1 (2016-06-21) -- "Bug in Your Hair"
Copyright (C) 2016 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> require(glmnet)
Loading required package: glmnet
Loading required package: Matrix
Loading required package: foreach
Loaded glmnet 2.0-5

> require(caret)
Loading required package: caret
Loading required package: lattice
Loading required package: ggplot2
> 
> ### Put directory of file location here
> dir <- '/home/egoren/'
> 
> ## ----data----------------------------------------------------------------
> train <- read.csv(paste0(dir, "train.csv"))
> test <- read.csv(paste0(dir, "test.csv"))
> testID <- test$Id
> test$SalePrice <- 0
> df.combined <- rbind(train, test)
> 
> 
> ## ----data2---------------------------------------------------------------
> df.combined$MSSubClass <- as.factor(as.numeric(df.combined$MSSubClass))
> 
> # Add indicator for missing garage year built
> df.combined$GarageMiss <- as.factor(is.na(df.combined$GarageYrBlt))
> 
> # Replace NA's.
> missing <- apply(df.combined, 2, function(x) sum(is.na(x)))
> missing[missing > 0]
    MSZoning  LotFrontage        Alley    Utilities  Exterior1st  Exterior2nd 
           4          486         2721            2            1            1 
  MasVnrType   MasVnrArea     BsmtQual     BsmtCond BsmtExposure BsmtFinType1 
          24           23           81           82           82           79 
  BsmtFinSF1 BsmtFinType2   BsmtFinSF2    BsmtUnfSF  TotalBsmtSF   Electrical 
           1           80            1            1            1            1 
BsmtFullBath BsmtHalfBath  KitchenQual   Functional  FireplaceQu   GarageType 
           2            2            1            2         1420          157 
 GarageYrBlt GarageFinish   GarageCars   GarageArea   GarageQual   GarageCond 
         159          159            1            1          159          159 
      PoolQC        Fence  MiscFeature     SaleType 
        2909         2348         2814            1 
> missvars <- names(missing[missing > 0])
> str(subset(df.combined, select = missvars))
'data.frame':	2919 obs. of  34 variables:
 $ MSZoning    : Factor w/ 5 levels "C (all)","FV",..: 4 4 4 4 4 4 4 4 5 4 ...
 $ LotFrontage : int  65 80 68 60 84 85 75 NA 51 50 ...
 $ Alley       : Factor w/ 2 levels "Grvl","Pave": NA NA NA NA NA NA NA NA NA NA ...
 $ Utilities   : Factor w/ 2 levels "AllPub","NoSeWa": 1 1 1 1 1 1 1 1 1 1 ...
 $ Exterior1st : Factor w/ 15 levels "AsbShng","AsphShn",..: 13 9 13 14 13 13 13 7 4 9 ...
 $ Exterior2nd : Factor w/ 16 levels "AsbShng","AsphShn",..: 14 9 14 16 14 14 14 7 16 9 ...
 $ MasVnrType  : Factor w/ 4 levels "BrkCmn","BrkFace",..: 2 3 2 3 2 3 4 4 3 3 ...
 $ MasVnrArea  : int  196 0 162 0 350 0 186 240 0 0 ...
 $ BsmtQual    : Factor w/ 4 levels "Ex","Fa","Gd",..: 3 3 3 4 3 3 1 3 4 4 ...
 $ BsmtCond    : Factor w/ 4 levels "Fa","Gd","Po",..: 4 4 4 2 4 4 4 4 4 4 ...
 $ BsmtExposure: Factor w/ 4 levels "Av","Gd","Mn",..: 4 2 3 4 1 4 1 3 4 4 ...
 $ BsmtFinType1: Factor w/ 6 levels "ALQ","BLQ","GLQ",..: 3 1 3 1 3 3 3 1 6 3 ...
 $ BsmtFinSF1  : int  706 978 486 216 655 732 1369 859 0 851 ...
 $ BsmtFinType2: Factor w/ 6 levels "ALQ","BLQ","GLQ",..: 6 6 6 6 6 6 6 2 6 6 ...
 $ BsmtFinSF2  : int  0 0 0 0 0 0 0 32 0 0 ...
 $ BsmtUnfSF   : int  150 284 434 540 490 64 317 216 952 140 ...
 $ TotalBsmtSF : int  856 1262 920 756 1145 796 1686 1107 952 991 ...
 $ Electrical  : Factor w/ 5 levels "FuseA","FuseF",..: 5 5 5 5 5 5 5 5 2 5 ...
 $ BsmtFullBath: int  1 0 1 1 1 1 1 1 0 1 ...
 $ BsmtHalfBath: int  0 1 0 0 0 0 0 0 0 0 ...
 $ KitchenQual : Factor w/ 4 levels "Ex","Fa","Gd",..: 3 4 3 3 3 4 3 4 4 4 ...
 $ Functional  : Factor w/ 7 levels "Maj1","Maj2",..: 7 7 7 7 7 7 7 7 3 7 ...
 $ FireplaceQu : Factor w/ 5 levels "Ex","Fa","Gd",..: NA 5 5 3 5 NA 3 5 5 5 ...
 $ GarageType  : Factor w/ 6 levels "2Types","Attchd",..: 2 2 2 6 2 2 2 2 6 2 ...
 $ GarageYrBlt : int  2003 1976 2001 1998 2000 1993 2004 1973 1931 1939 ...
 $ GarageFinish: Factor w/ 3 levels "Fin","RFn","Unf": 2 2 2 3 2 3 2 2 3 2 ...
 $ GarageCars  : int  2 2 2 3 3 2 2 2 2 1 ...
 $ GarageArea  : int  548 460 608 642 836 480 636 484 468 205 ...
 $ GarageQual  : Factor w/ 5 levels "Ex","Fa","Gd",..: 5 5 5 5 5 5 5 5 2 3 ...
 $ GarageCond  : Factor w/ 5 levels "Ex","Fa","Gd",..: 5 5 5 5 5 5 5 5 5 5 ...
 $ PoolQC      : Factor w/ 3 levels "Ex","Fa","Gd": NA NA NA NA NA NA NA NA NA NA ...
 $ Fence       : Factor w/ 4 levels "GdPrv","GdWo",..: NA NA NA NA NA 3 NA NA NA NA ...
 $ MiscFeature : Factor w/ 4 levels "Gar2","Othr",..: NA NA NA NA NA 3 NA 3 NA NA ...
 $ SaleType    : Factor w/ 9 levels "COD","Con","ConLD",..: 9 9 9 9 9 9 9 9 9 9 ...
> # Add NA as a factor level for categorical variables
> for (i in 1:length(missvars)) {
+     thisvar <- df.combined[ , missvars[i]]
+     if (is.factor(thisvar))
+         thisvar <- addNA(thisvar)
+     df.combined[ , missvars[i]] <- thisvar
+ }
> # Make indicators for conditions.
> options(na.action = 'na.pass')
> cond1 <- data.frame(model.matrix(~ Condition1 + 0, data = df.combined))
> names(cond1) <- sub(".*1", "", names(cond1))
> cond2 <- data.frame(model.matrix(~ Condition2 + 0, data = df.combined))
> names(cond2) <- sub(".*2", "", names(cond2))
> idx <- names(cond1) %in% names(cond2)
> cond <- cond1
> cond[, idx] <- cond1[, idx] + cond2
> cond <- ifelse(cond == 0, 0, 1)
> cond <- as.data.frame(t(apply(cond, 1, as.factor)))
> df.combined <- subset(df.combined, select = -c(Condition1, Condition2))
> df.combined <- cbind(df.combined, cond)
> df.proc <- preProcess(df.combined, method = 'medianImpute')
> df.proc <- predict(df.proc, df.combined)
> df.proc$SalePrice[df.proc$SalePrice == 0] <- NA
> df.proc$SalePrice <- log(df.proc$SalePrice)
> 
> ## ----mod------------------------------------------------------------
> d <- df.proc
> X <- model.matrix(SalePrice ~ (.)^2 + 0, data = d)
> train.idx <- !is.na(d$SalePrice)
> set.seed(602)
> fit <- train(x = X[train.idx,],
+              y = d$SalePrice[train.idx],
+              method = "glmnet",
+              standardize.response = TRUE, standardize = TRUE,
+              #preProcess = c("zv"),
+              trControl = trainControl(method = 'repeatedcv',
+                                       number = 10, repeats = 10),
+              tuneGrid = expand.grid(.alpha = seq(0, 1, by = 0.1),
+                                     .lambda = seq(0.001, 1, by = 0.05)))
Warning message:
In nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo,  :
  There were missing values in resampled performance measures.
> fit$bestTune
   alpha lambda
20     0  0.951
> fit$results
    alpha lambda      RMSE  Rsquared     RMSESD RsquaredSD
1     0.0  0.001 0.1418301 0.8725118 0.03780592 0.06835008
2     0.0  0.051 0.1418301 0.8725118 0.03780592 0.06835008
3     0.0  0.101 0.1418301 0.8725118 0.03780592 0.06835008
4     0.0  0.151 0.1418301 0.8725118 0.03780592 0.06835008
5     0.0  0.201 0.1418301 0.8725118 0.03780592 0.06835008
6     0.0  0.251 0.1418301 0.8725118 0.03780592 0.06835008
7     0.0  0.301 0.1418301 0.8725118 0.03780592 0.06835008
8     0.0  0.351 0.1418301 0.8725118 0.03780592 0.06835008
9     0.0  0.401 0.1418301 0.8725118 0.03780592 0.06835008
10    0.0  0.451 0.1418301 0.8725118 0.03780592 0.06835008
11    0.0  0.501 0.1418301 0.8725118 0.03780592 0.06835008
12    0.0  0.551 0.1418301 0.8725118 0.03780592 0.06835008
13    0.0  0.601 0.1418301 0.8725118 0.03780592 0.06835008
14    0.0  0.651 0.1418301 0.8725118 0.03780592 0.06835008
15    0.0  0.701 0.1418301 0.8725118 0.03780592 0.06835008
16    0.0  0.751 0.1418301 0.8725118 0.03780592 0.06835008
17    0.0  0.801 0.1418301 0.8725118 0.03780592 0.06835008
18    0.0  0.851 0.1418301 0.8725118 0.03780592 0.06835008
19    0.0  0.901 0.1418301 0.8725118 0.03780592 0.06835008
20    0.0  0.951 0.1418301 0.8725118 0.03780592 0.06835008
21    0.1  0.001 0.1472288 0.8625625 0.05044395 0.08953834
22    0.1  0.051 0.1453615 0.8654474 0.04932672 0.08897448
23    0.1  0.101 0.1453062 0.8660986 0.04561751 0.08673899
24    0.1  0.151 0.1493498 0.8606707 0.04326182 0.08682242
25    0.1  0.201 0.1531020 0.8564168 0.04045228 0.08543037
26    0.1  0.251 0.1569922 0.8523882 0.03778535 0.08391630
27    0.1  0.301 0.1610347 0.8486657 0.03523937 0.08240916
28    0.1  0.351 0.1651130 0.8451630 0.03324572 0.08177885
29    0.1  0.401 0.1691237 0.8418106 0.03146121 0.08125798
30    0.1  0.451 0.1731610 0.8385595 0.02976576 0.08062270
31    0.1  0.501 0.1772404 0.8355004 0.02820435 0.08001137
32    0.1  0.551 0.1814105 0.8325158 0.02678420 0.07949872
33    0.1  0.601 0.1856734 0.8295434 0.02545235 0.07891514
34    0.1  0.651 0.1900178 0.8265631 0.02421769 0.07824799
35    0.1  0.701 0.1944203 0.8235988 0.02309533 0.07753940
36    0.1  0.751 0.1988268 0.8208077 0.02209392 0.07687684
37    0.1  0.801 0.2031938 0.8183561 0.02124397 0.07634809
38    0.1  0.851 0.2075613 0.8161466 0.02053590 0.07597303
39    0.1  0.901 0.2119888 0.8139487 0.01993686 0.07566480
40    0.1  0.951 0.2164808 0.8117164 0.01943412 0.07538154
41    0.2  0.001 0.1487390 0.8602314 0.05200856 0.09195827
42    0.2  0.051 0.1459998 0.8648093 0.04639431 0.08771020
43    0.2  0.101 0.1528987 0.8560743 0.04114314 0.08584503
44    0.2  0.151 0.1602779 0.8485723 0.03592350 0.08255033
45    0.2  0.201 0.1678574 0.8417211 0.03207741 0.08108809
46    0.2  0.251 0.1755805 0.8352268 0.02850397 0.07896015
47    0.2  0.301 0.1837918 0.8284267 0.02544811 0.07676614
48    0.2  0.351 0.1923626 0.8212218 0.02291735 0.07446252
49    0.2  0.401 0.2007877 0.8149437 0.02115617 0.07320884
50    0.2  0.451 0.2091113 0.8096330 0.01995761 0.07282684
51    0.2  0.501 0.2175031 0.8047023 0.01904068 0.07221334
52    0.2  0.551 0.2260257 0.7999389 0.01840314 0.07154287
53    0.2  0.601 0.2346500 0.7954500 0.01803019 0.07084505
54    0.2  0.651 0.2433344 0.7912508 0.01783923 0.06976992
55    0.2  0.701 0.2520457 0.7874668 0.01784509 0.06856394
56    0.2  0.751 0.2607088 0.7844112 0.01800110 0.06738772
57    0.2  0.801 0.2693595 0.7819113 0.01828426 0.06609576
58    0.2  0.851 0.2780911 0.7792573 0.01864131 0.06466281
59    0.2  0.901 0.2868613 0.7763350 0.01903963 0.06311279
60    0.2  0.951 0.2956290 0.7732268 0.01946617 0.06148747
61    0.3  0.001 0.1493189 0.8593487 0.05249539 0.09271640
62    0.3  0.051 0.1494257 0.8600876 0.04395553 0.08706029
63    0.3  0.101 0.1601563 0.8481379 0.03624934 0.08270397
64    0.3  0.151 0.1711576 0.8381050 0.03046515 0.07988008
65    0.3  0.201 0.1831176 0.8275769 0.02574309 0.07671591
66    0.3  0.251 0.1955385 0.8168828 0.02207894 0.07308619
67    0.3  0.301 0.2076482 0.8083984 0.01991786 0.07134406
68    0.3  0.351 0.2202110 0.7996648 0.01865291 0.07020352
69    0.3  0.401 0.2328958 0.7916732 0.01796245 0.06892617
70    0.3  0.451 0.2455678 0.7849433 0.01773190 0.06680056
71    0.3  0.501 0.2580757 0.7804458 0.01792668 0.06505044
72    0.3  0.551 0.2707998 0.7758646 0.01833682 0.06313192
73    0.3  0.601 0.2836061 0.7711603 0.01890868 0.06096749
74    0.3  0.651 0.2965160 0.7658496 0.01955750 0.05868116
75    0.3  0.701 0.3094215 0.7595135 0.02022790 0.05645984
76    0.3  0.751 0.3221905 0.7521855 0.02088267 0.05381087
77    0.3  0.801 0.3344819 0.7471366 0.02144427 0.05153416
78    0.3  0.851 0.3464933 0.7436995 0.02207388 0.04894362
79    0.3  0.901 0.3584283 0.7373078 0.02271701 0.04672600
80    0.3  0.951 0.3701215 0.7251781 0.02328553 0.04507848
81    0.4  0.001 0.1496878 0.8587777 0.05262913 0.09289478
82    0.4  0.051 0.1531412 0.8552162 0.04121558 0.08528569
83    0.4  0.101 0.1671248 0.8413589 0.03221684 0.08031086
84    0.4  0.151 0.1825408 0.8275047 0.02596033 0.07670525
85    0.4  0.201 0.1987595 0.8135845 0.02149727 0.07272983
86    0.4  0.251 0.2151623 0.8011184 0.01905727 0.07026431
87    0.4  0.301 0.2318511 0.7895990 0.01789342 0.06747010
88    0.4  0.351 0.2483762 0.7807201 0.01772416 0.06487742
89    0.4  0.401 0.2648173 0.7742143 0.01814515 0.06251712
90    0.4  0.451 0.2815377 0.7673613 0.01885279 0.06000015
91    0.4  0.501 0.2984660 0.7594182 0.01969498 0.05673747
92    0.4  0.551 0.3153032 0.7515673 0.02053196 0.05323881
93    0.4  0.601 0.3317548 0.7468222 0.02135802 0.04989442
94    0.4  0.651 0.3482010 0.7398959 0.02223869 0.04734800
95    0.4  0.701 0.3643519 0.7259481 0.02308081 0.04499443
96    0.4  0.751 0.3796845 0.7066212 0.02379344 0.04342910
97    0.4  0.801 0.3935252 0.6964082 0.02426425 0.04880042
98    0.4  0.851 0.3987151       NaN 0.02292270         NA
99    0.4  0.901 0.3987151       NaN 0.02292270         NA
100   0.4  0.951 0.3987151       NaN 0.02292270         NA
101   0.5  0.001 0.1497529 0.8587154 0.05264556 0.09288163
102   0.5  0.051 0.1565768 0.8511971 0.03829545 0.08280507
103   0.5  0.101 0.1742730 0.8345918 0.02894555 0.07836140
104   0.5  0.151 0.1941821 0.8166121 0.02247259 0.07329006
105   0.5  0.201 0.2146955 0.7996445 0.01922421 0.07073381
106   0.5  0.251 0.2354970 0.7841769 0.01782482 0.06671012
107   0.5  0.301 0.2555415 0.7746764 0.01790567 0.06338876
108   0.5  0.351 0.2758593 0.7665063 0.01864673 0.05958928
109   0.5  0.401 0.2966904 0.7575534 0.01966022 0.05531255
110   0.5  0.451 0.3175185 0.7497805 0.02066567 0.05122375
111   0.5  0.501 0.3382321 0.7429660 0.02174180 0.04802510
112   0.5  0.551 0.3588834 0.7272788 0.02284490 0.04553016
113   0.5  0.601 0.3787077 0.7067461 0.02382628 0.04401718
114   0.5  0.651 0.3964671 0.6872397 0.02433769 0.04700299
115   0.5  0.701 0.3987151       NaN 0.02292270         NA
116   0.5  0.751 0.3987151       NaN 0.02292270         NA
117   0.5  0.801 0.3987151       NaN 0.02292270         NA
118   0.5  0.851 0.3987151       NaN 0.02292270         NA
119   0.5  0.901 0.3987151       NaN 0.02292270         NA
120   0.5  0.951 0.3987151       NaN 0.02292270         NA
121   0.6  0.001 0.1499353 0.8584528 0.05258026 0.09267413
122   0.6  0.051 0.1597965 0.8479925 0.03603921 0.08146360
123   0.6  0.101 0.1816258 0.8276956 0.02593042 0.07573775
124   0.6  0.151 0.2060209 0.8057607 0.02039237 0.07173854
125   0.6  0.201 0.2309647 0.7850988 0.01801791 0.06707546
126   0.6  0.251 0.2545944 0.7730791 0.01792667 0.06232503
127   0.6  0.301 0.2787093 0.7637419 0.01882358 0.05758098
128   0.6  0.351 0.3036426 0.7528814 0.02001155 0.05301105
129   0.6  0.401 0.3284048 0.7460877 0.02126395 0.04887980
130   0.6  0.451 0.3536188 0.7288111 0.02264532 0.04562668
131   0.6  0.501 0.3779808 0.7073044 0.02384599 0.04498255
132   0.6  0.551 0.3984255 0.6501403 0.02335819 0.03926473
133   0.6  0.601 0.3987151       NaN 0.02292270         NA
134   0.6  0.651 0.3987151       NaN 0.02292270         NA
135   0.6  0.701 0.3987151       NaN 0.02292270         NA
136   0.6  0.751 0.3987151       NaN 0.02292270         NA
137   0.6  0.801 0.3987151       NaN 0.02292270         NA
138   0.6  0.851 0.3987151       NaN 0.02292270         NA
139   0.6  0.901 0.3987151       NaN 0.02292270         NA
140   0.6  0.951 0.3987151       NaN 0.02292270         NA
141   0.7  0.001 0.1502130 0.8579858 0.05279137 0.09307750
142   0.7  0.051 0.1630311 0.8447808 0.03379798 0.07981526
143   0.7  0.101 0.1894888 0.8197399 0.02351018 0.07327144
144   0.7  0.151 0.2182632 0.7938007 0.01902805 0.07016647
145   0.7  0.201 0.2461097 0.7748030 0.01783351 0.06336313
146   0.7  0.251 0.2735031 0.7645041 0.01866665 0.05833986
147   0.7  0.301 0.3022978 0.7526823 0.01995792 0.05278619
148   0.7  0.351 0.3313776 0.7434728 0.02148898 0.04843998
149   0.7  0.401 0.3610409 0.7188791 0.02307103 0.04446226
150   0.7  0.451 0.3893072 0.6918192 0.02432137 0.04965126
151   0.7  0.501 0.3987151       NaN 0.02292270         NA
152   0.7  0.551 0.3987151       NaN 0.02292270         NA
153   0.7  0.601 0.3987151       NaN 0.02292270         NA
154   0.7  0.651 0.3987151       NaN 0.02292270         NA
155   0.7  0.701 0.3987151       NaN 0.02292270         NA
156   0.7  0.751 0.3987151       NaN 0.02292270         NA
157   0.7  0.801 0.3987151       NaN 0.02292270         NA
158   0.7  0.851 0.3987151       NaN 0.02292270         NA
159   0.7  0.901 0.3987151       NaN 0.02292270         NA
160   0.7  0.951 0.3987151       NaN 0.02292270         NA
161   0.8  0.001 0.1504277 0.8576446 0.05244622 0.09239134
162   0.8  0.051 0.1663368 0.8414308 0.03185676 0.07834650
163   0.8  0.101 0.1973465 0.8117706 0.02200282 0.07261212
164   0.8  0.151 0.2299253 0.7832278 0.01822234 0.06739232
165   0.8  0.201 0.2605108 0.7688534 0.01825855 0.06073138
166   0.8  0.251 0.2929943 0.7548610 0.01959169 0.05465672
167   0.8  0.301 0.3261669 0.7446578 0.02125816 0.04936185
168   0.8  0.351 0.3602237 0.7186393 0.02306562 0.04511252
169   0.8  0.401 0.3927866 0.6876480 0.02454625 0.05138086
170   0.8  0.451 0.3987151       NaN 0.02292270         NA
171   0.8  0.501 0.3987151       NaN 0.02292270         NA
172   0.8  0.551 0.3987151       NaN 0.02292270         NA
173   0.8  0.601 0.3987151       NaN 0.02292270         NA
174   0.8  0.651 0.3987151       NaN 0.02292270         NA
175   0.8  0.701 0.3987151       NaN 0.02292270         NA
176   0.8  0.751 0.3987151       NaN 0.02292270         NA
177   0.8  0.801 0.3987151       NaN 0.02292270         NA
178   0.8  0.851 0.3987151       NaN 0.02292270         NA
179   0.8  0.901 0.3987151       NaN 0.02292270         NA
180   0.8  0.951 0.3987151       NaN 0.02292270         NA
181   0.9  0.001 0.1512129 0.8563019 0.05261893 0.09270434
182   0.9  0.051 0.1697472 0.8380262 0.02998714 0.07663334
183   0.9  0.101 0.2052679 0.8031573 0.02070890 0.07153100
184   0.9  0.151 0.2407470 0.7755954 0.01802140 0.06415843
185   0.9  0.201 0.2757076 0.7615488 0.01903686 0.05850123
186   0.9  0.251 0.3125372 0.7490048 0.02057066 0.05128210
187   0.9  0.301 0.3509593 0.7228252 0.02264211 0.04612565
188   0.9  0.351 0.3887597 0.6864085 0.02442068 0.04974359
189   0.9  0.401 0.3987151       NaN 0.02292270         NA
190   0.9  0.451 0.3987151       NaN 0.02292270         NA
191   0.9  0.501 0.3987151       NaN 0.02292270         NA
192   0.9  0.551 0.3987151       NaN 0.02292270         NA
193   0.9  0.601 0.3987151       NaN 0.02292270         NA
194   0.9  0.651 0.3987151       NaN 0.02292270         NA
195   0.9  0.701 0.3987151       NaN 0.02292270         NA
196   0.9  0.751 0.3987151       NaN 0.02292270         NA
197   0.9  0.801 0.3987151       NaN 0.02292270         NA
198   0.9  0.851 0.3987151       NaN 0.02292270         NA
199   0.9  0.901 0.3987151       NaN 0.02292270         NA
200   0.9  0.951 0.3987151       NaN 0.02292270         NA
201   1.0  0.001 0.1531192 0.8532158 0.05324154 0.09339769
202   1.0  0.051 0.1729134 0.8352291 0.02799824 0.07416075
203   1.0  0.101 0.2130014 0.7942681 0.01961233 0.06941851
204   1.0  0.151 0.2508545 0.7715314 0.01816618 0.06167950
205   1.0  0.201 0.2907118 0.7536732 0.01950249 0.05406990
206   1.0  0.251 0.3326063 0.7336840 0.02170477 0.04972374
207   1.0  0.301 0.3756698 0.6956231 0.02384502 0.05055572
208   1.0  0.351 0.3987151       NaN 0.02292270         NA
209   1.0  0.401 0.3987151       NaN 0.02292270         NA
210   1.0  0.451 0.3987151       NaN 0.02292270         NA
211   1.0  0.501 0.3987151       NaN 0.02292270         NA
212   1.0  0.551 0.3987151       NaN 0.02292270         NA
213   1.0  0.601 0.3987151       NaN 0.02292270         NA
214   1.0  0.651 0.3987151       NaN 0.02292270         NA
215   1.0  0.701 0.3987151       NaN 0.02292270         NA
216   1.0  0.751 0.3987151       NaN 0.02292270         NA
217   1.0  0.801 0.3987151       NaN 0.02292270         NA
218   1.0  0.851 0.3987151       NaN 0.02292270         NA
219   1.0  0.901 0.3987151       NaN 0.02292270         NA
220   1.0  0.951 0.3987151       NaN 0.02292270         NA
> hat.train <- predict(fit$finalModel, X[train.idx,])
> hat.test <- predict(fit$finalModel, X[!train.idx,])
> 
> fit2 <- glmnet(x = X[train.idx,],
+                y = d$SalePrice[train.idx],
+                standardize.response = TRUE, standardize = TRUE,
+                lambda = fit$bestTune$lambda,
+                alpha = fit$bestTune$alpha)
> 
> save.image('/home/egoren/el.Rdat')
> 
> 
> proc.time()
    user   system  elapsed 
7253.427  372.613 7631.042 
