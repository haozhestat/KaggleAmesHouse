Expec_Inv_Z
IMH_sample <- IMH(10000,1,1,0.7)
mean(IMH_sample) - Expec_Z
mean(1/IMH_sample) - Expec_Inv_Z
abs(mean(IMH_sample) - Expec_Z)/Expec_Z
abs(mean(1/IMH_sample) - Expec_Inv_Z)/Expec_Inv_Z
?rgamma
?rbeta
gibsamp_1_1 <- Gibbs_Sampler(1000,X,Y,1,1,2,0.3,R)
gibsamp_1_1$beta
gibsamp_1_1$lambda
gibsamp_1_1[[1]]
mean(gibsamp_1_1[[1]])
mean(gibsamp_1_1[[2]])
theta_1 <- 1.5
theta_2 <- 2
Expec_Z <- sqrt(theta_2/theta_1)
Expec_Inv_Z <- sqrt(theta_1/theta_2) + 1/(2*theta_2)
f <- function(z){
z^{-1.5}*exp(-theta_1*z-theta_2/z+2*sqrt(theta_1*theta_2)+
log(sqrt(2*theta_2)))
}
IMH <- function(m, gamma_shape, gamma_rate, init){
X <- rep(NA, m)
X[1] <- init
for(i in 2:m){
U <- runif(1,0,1)
Y <- rgamma(1,gamma_shape, gamma_rate)
r <- min(f(Y)/f(X[i-1])*dgamma(X[i-1],gamma_shape, gamma_rate)/
dgamma(Y,gamma_shape, gamma_rate),1)
if(U<=r) X[i] <- Y
else X[i] <- X[i-1]
}
return(X)
}
IMH_sample <- IMH(10000,1,1,0.7)
abs(mean(IMH_sample) - Expec_Z)/Expec_Z
abs(mean(1/IMH_sample) - Expec_Inv_Z)/Expec_Inv_Z
abs(mean(IMH_sample) - Expec_Z)
abs(mean(1/IMH_sample) - Expec_Inv_Z)
IMH_sample <- IMH(10000,1,10,0.7)
abs(mean(IMH_sample) - Expec_Z)
abs(mean(1/IMH_sample) - Expec_Inv_Z)
IMH_sample <- IMH(10000,10,1,0.7)
abs(mean(IMH_sample) - Expec_Z)
abs(mean(1/IMH_sample) - Expec_Inv_Z)
Expec_Z
IMH_sample <- IMH(10000,10,10,0.7)
abs(mean(IMH_sample) - Expec_Z)
abs(mean(1/IMH_sample) - Expec_Inv_Z)
IMH_sample <- IMH(10000,0.1,0.1,0.7)
abs(mean(IMH_sample) - Expec_Z)
abs(mean(1/IMH_sample) - Expec_Inv_Z)
abs(mean(IMH_sample) - Expec_Z)
abs(mean(1/IMH_sample) - Expec_Inv_Z)
p <- 0.3
lambda <- 2
n <- 100
set.seed(580)
Y <- rpois(n, lambda)
R <- rbinom(n, 1, p)
X <- R*Y
X
plot(density(X))
plot(hist(X), main="The histogram of X")
n <- 10000
plot(hist(X), main="The histogram of X")
p <- 0.3
lambda <- 2
n <- 10000
set.seed(580)
Y <- rpois(n, lambda)
R <- rbinom(n, 1, p)
X <- R*Y
X
plot(hist(X), main="The histogram of X")
?hist
plot(hist(X,probability = TRUE), main="The histogram of X")
plot(hist(X, freq = FALSE), main="The histogram of X")
plot(hist(X, freq = TRUE), main="The histogram of X")
p <- 0.3
lambda <- 2
n <- 100
set.seed(580)
Y <- rpois(n, lambda)
R <- rbinom(n, 1, p)
X <- R*Y
X
plot(hist(X, freq = TRUE), main="The histogram of X")
gibsamp_1_1 <- Gibbs_Sampler(1000,X,Y,1,1,2,0.3,R)
quantile(gibsamp_1_1[[1]], c(0.025,0.975))
quantile(gibsamp_1_1[[2]], c(0.025,0.975))
gibsamp_1_1 <- Gibbs_Sampler(1000,X,Y,1,10,2,0.3,R)
gibsamp_1_1 <- Gibbs_Sampler(10000,X,Y,1,10,2,0.3,R)
quantile(gibsamp_1_1[[1]], c(0.025,0.975))
quantile(gibsamp_1_1[[2]], c(0.025,0.975))
gibsamp_1_1 <- Gibbs_Sampler(10000,X,Y,1,1,2,0.3,R)
quantile(gibsamp_1_1[[1]], c(0.025,0.975))
quantile(gibsamp_1_1[[2]], c(0.025,0.975))
gibsamp_1_1 <- Gibbs_Sampler(10000,X,Y,10,1,2,0.3,R)
quantile(gibsamp_1_1[[1]], c(0.025,0.975))
quantile(gibsamp_1_1[[2]], c(0.025,0.975))
gibsamp_1_1 <- Gibbs_Sampler(10000,X,Y,10,10,2,0.3,R)
quantile(gibsamp_1_1[[1]], c(0.025,0.975))
quantile(gibsamp_1_1[[2]], c(0.025,0.975))
setwd("~/Dropbox/STAT602/hw3/")
library(GGally)
library(mclust)
library(reshape2)
library(kernlab)
library(phyclust)
library(MASS)
library(caret)
library(rpart)
library(psych)
library(fastAdaboost)
library(bst)
library(nnet)
library(randomForest)
library(glmnet)
library(dplyr)
library(ggplot2)
library(xgboost)
library(klaR)
baseballdat <- read.csv("MaxHRperYear.csv", header = TRUE)
dim(baseballdat)
X <- baseballdat$X
Y <- baseballdat$max
Kernel1_fun <- function(x,z){
exp(-0.5*(x-z)^2)
}
Kernel2_fun <- function(x,z){
exp(-(x-z)^2)
}
K_mat <- function(kernel_fun){
sapply(X, function(z) kernel_fun(z,X))
}
A_mat_1 <-  cbind(rep(1,length(X)), X, X^2, K_mat(Kernel1_fun))
A_mat_2 <-  cbind(rep(1,length(X)), X, X^2, K_mat(Kernel2_fun))
B_mat_1 <- diag(0,3+length(X))
B_mat_2 <- diag(0,3+length(X))
B_mat_1[4:(length(X)+3),4:(length(X)+3)] <- K_mat(Kernel1_fun)
B_mat_2[4:(length(X)+3),4:(length(X)+3)] <- K_mat(Kernel2_fun)
lambda1 <- 0.01
lambda2 <- 100
yhat_fun_1 <- function(x){
nu <- matrix(c(1,x,x^2,sapply(X, function(z) Kernel1_fun(z,x))),1,3+length(X))
nu%*%ginv(t(A_mat_1)%*%A_mat_1+lambda1*B_mat_1)%*%t(A_mat_1)%*%Y
}
yhat_fun_2 <- function(x){
nu <- matrix(c(1,x,x^2,sapply(X, function(z) Kernel1_fun(z,x))),1,3+length(X))
nu%*%ginv(t(A_mat_1)%*%A_mat_1+lambda2*B_mat_1)%*%t(A_mat_1)%*%Y
}
yhat_fun_3 <- function(x){
nu <- matrix(c(1,x,x^2,sapply(X, function(z) Kernel2_fun(z,x))),1,3+length(X))
nu%*%ginv(t(A_mat_2)%*%A_mat_2+lambda1*B_mat_2)%*%t(A_mat_2)%*%Y
}
yhat_fun_4 <- function(x){
nu <- matrix(c(1,x,x^2,sapply(X, function(z) Kernel2_fun(z,x))),1,3+length(X))
nu%*%ginv(t(A_mat_2)%*%A_mat_2+lambda2*B_mat_2)%*%t(A_mat_2)%*%Y
}
par(mfrow=c(2,2))
plot(X,Y, main="lambda = 1, kernel function 1")
lines(X,Vectorize(yhat_fun_1)(X))
plot(X,Y, main="lambda = 10, kernel function 1")
lines(X,Vectorize(yhat_fun_2)(X))
plot(X,Y, main="lambda = 1, kernel function 2")
lines(X,Vectorize(yhat_fun_3)(X))
plot(X,Y, main="lambda = 10, kernel function 2")
lines(X,Vectorize(yhat_fun_4)(X))
plot(X,Y, main="lambda = 1, kernel function 1")
plot(X,Y, main="lambda = 1, kernel function 1")
lines(X,Vectorize(yhat_fun_1)(X))
plot(X,Y, main="lambda = 10, kernel function 1")
lines(X,Vectorize(yhat_fun_2)(X))
plot(X,Y, main="lambda = 1, kernel function 2")
lines(X,Vectorize(yhat_fun_3)(X))
plot(X,Y, main="lambda = 10, kernel function 2")
lines(X,Vectorize(yhat_fun_4)(X))
plot(X,Y, main="lambda = 1, kernel function 1")
lines(X,Vectorize(yhat_fun_1)(X))
lines(X,Vectorize(yhat_fun_1)(X))
lines(X,Vectorize(yhat_fun_2)(X))
plot(X,Y, main="lambda = 1, kernel function 1")
plot(X,Y, main="lambda = 1, kernel function 1")
lines(X,Vectorize(yhat_fun_1)(X))
#plot(X,Y, main="lambda = 10, kernel function 1")
lines(X,Vectorize(yhat_fun_2)(X))
#plot(X,Y, main="lambda = 1, kernel function 2")
lines(X,Vectorize(yhat_fun_3)(X))
#plot(X,Y, main="lambda = 10, kernel function 2")
lines(X,Vectorize(yhat_fun_4)(X))
library("xgboost")
rm(list = ls())
setwd("~/Documents/KaggleAmesHouse/xgboost_tune")
library(caret)
library(xgboost)
df <- read.csv("featureMat_v2.csv", header = TRUE, stringsAsFactors = FALSE)
x_train = df[df$Train == 1, 3:ncol(df)]
x_test = df[df$Train == 0, 3:ncol(df)]
y_train = df$SalePrice[df$Train == 1]
dtrain = xgb.DMatrix(as.matrix(x_train), label = y_train)
dtest = xgb.DMatrix(as.matrix(x_test))
xgb_params = list(
booster = 'gbtree',
#objective = 'reg:linear',
colsample_bytree=0.6,
eta=0.005,
max_depth=8,
min_child_weight=2,
#alpha=0.3,
#lambda=0.4,
gamma=0.01, # less overfit
subsample=0.5,
silent=TRUE)
bst = xgb.train(xgb_params,dtrain, nrounds = 20000)
y_pred.xgb = predict(bst, dtest)
y_train.xgb = predict(bst, dtrain)
y_pred <- data.frame(Id = 1461:2919 , SalePrice = exp(y_pred.xgb))
y_pred_train <- data.frame(Id = setdiff(1:1460, c(524,692,1183,1299)),
SalePrice = exp(y_train.xgb))
write.csv(y_pred, file="xgboost_test_fm2.csv", row.names = FALSE)
write.csv(y_pred_train, file="xgboost_train_fm2.csv", row.names = FALSE)
rm(list = ls())
setwd("~/Documents/KaggleAmesHouse/stacking")
library(glmnet)
pls_train <- read.csv("pls_train_fm2.csv", header = TRUE)
pls_test <- read.csv("pls_test_fm2.csv", header = TRUE)
rf_train <- read.csv("RFTrainPred.csv", header = TRUE)
rf_test <- read.csv("RFTestPred.csv", header = TRUE)
xgb_train <- read.csv("xgboost_train_fm2.csv", header = TRUE)
xgb_test <- read.csv("xgboost_test_fm2.csv", header = TRUE)
elastic <- read.csv("el-EMG-pred-CV-ALL.csv", header = TRUE)
elastic_train <- elastic[1:1460,]
elastic_test <- elastic[1461:2919,]
ridge_train <- read.csv("ridge_train_fm2.csv", header=TRUE)
ridge_test <- read.csv("ridge_test_fm2.csv", header=TRUE)
x_train <- data.frame(pls = pls_train$SalePrice[pls_train$Id %in% xgb_train$Id],
#rf = rf_train$SalePrice[rf_train$Id %in% xgb_train$Id],
xgb = xgb_train$SalePrice,
ridge = ridge_train$SalePrice)
x_train <- log(x_train)
x_test <- data.frame(pls = pls_test$SalePrice[pls_test$Id %in% xgb_test$Id],
#rf = rf_test$SalePrice[rf_test$Id %in% xgb_test$Id],
xgb = xgb_test$SalePrice,
ridge = ridge_test$SalePrice)
x_test <- log(x_test)
df <- read.csv("featureMat_v2.csv", header = TRUE, stringsAsFactors = FALSE)
y_train = df$SalePrice[df$Train == 1]
#y_train <- exp(y_train)
glmnet_cv <- cv.glmnet(x=as.matrix(x_train), y=y_train,alpha=0)
lasso_fit <- glmnet(x=as.matrix(x_train), y=y_train, alpha=0, lambda = glmnet_cv$lambda.1se)
lasso_fit$beta
head(ridge_train)
ridge_train <- read.csv("ridge_train_fm2.csv", header=TRUE, sep="\t")
ridge_test <- read.csv("ridge_test_fm2.csv", header=TRUE, sep="\t")
x_train <- data.frame(pls = pls_train$SalePrice[pls_train$Id %in% xgb_train$Id],
#rf = rf_train$SalePrice[rf_train$Id %in% xgb_train$Id],
xgb = xgb_train$SalePrice,
ridge = ridge_train$SalePrice)
x_train <- log(x_train)
x_test <- data.frame(pls = pls_test$SalePrice[pls_test$Id %in% xgb_test$Id],
#rf = rf_test$SalePrice[rf_test$Id %in% xgb_test$Id],
xgb = xgb_test$SalePrice,
ridge = ridge_test$SalePrice)
x_test <- log(x_test)
df <- read.csv("featureMat_v2.csv", header = TRUE, stringsAsFactors = FALSE)
y_train = df$SalePrice[df$Train == 1]
glmnet_cv <- cv.glmnet(x=as.matrix(x_train), y=y_train,alpha=0)
lasso_fit <- glmnet(x=as.matrix(x_train), y=y_train, alpha=0, lambda = glmnet_cv$lambda.1se)
lasso_fit$beta
plot(predict(lasso_fit,as.matrix(x_train)), y_train)
abline(a=0,b=1,col="red")
y_pred <- data.frame(Id = 1461:2919,
SalePrice = exp(predict(lasso_fit, as.matrix(x_test))))
colnames(y_pred) <- c("Id", "SalePrice")
write.csv(y_pred, file="stacking_lasso.csv", row.names = FALSE)
rm(list = ls())
setwd("~/Documents/KaggleAmesHouse/xgboost_tune")
library(caret)
library(xgboost)
df <- read.csv("featureMat_v2.csv", header = TRUE, stringsAsFactors = FALSE)
x_train = df[df$Train == 1, 3:ncol(df)]
x_test = df[df$Train == 0, 3:ncol(df)]
y_train = df$SalePrice[df$Train == 1]
dtrain = xgb.DMatrix(as.matrix(x_train), label = y_train)
dtest = xgb.DMatrix(as.matrix(x_test))
xgb_params = list(
booster = 'gbtree',
#objective = 'reg:linear',
colsample_bytree=0.6,
eta=0.005,
max_depth=8,
min_child_weight=2,
#alpha=0.3,
#lambda=0.4,
gamma=0.01, # less overfit
subsample=0.5,
silent=TRUE)
bst = xgb.train(xgb_params,dtrain, nrounds = 5000)
y_pred.xgb = predict(bst, dtest)
y_train.xgb = predict(bst, dtrain)
y_pred <- data.frame(Id = 1461:2919 , SalePrice = exp(y_pred.xgb))
y_pred_train <- data.frame(Id = setdiff(1:1460, c(524,692,1183,1299)),
SalePrice = exp(y_train.xgb))
write.csv(y_pred, file="xgboost_test_fm2.csv", row.names = FALSE)
write.csv(y_pred_train, file="xgboost_train_fm2.csv", row.names = FALSE)
rm(list = ls())
setwd("~/Documents/KaggleAmesHouse/stacking")
library(glmnet)
pls_train <- read.csv("pls_train_fm2.csv", header = TRUE)
pls_test <- read.csv("pls_test_fm2.csv", header = TRUE)
rf_train <- read.csv("RFTrainPred.csv", header = TRUE)
rf_test <- read.csv("RFTestPred.csv", header = TRUE)
xgb_train <- read.csv("xgboost_train_fm2.csv", header = TRUE)
xgb_test <- read.csv("xgboost_test_fm2.csv", header = TRUE)
elastic <- read.csv("el-EMG-pred-CV-ALL.csv", header = TRUE)
elastic_train <- elastic[1:1460,]
elastic_test <- elastic[1461:2919,]
ridge_train <- read.csv("ridge_train_fm2.csv", header=TRUE, sep="\t")
ridge_test <- read.csv("ridge_test_fm2.csv", header=TRUE, sep="\t")
x_train <- data.frame(pls = pls_train$SalePrice[pls_train$Id %in% xgb_train$Id],
#rf = rf_train$SalePrice[rf_train$Id %in% xgb_train$Id],
xgb = xgb_train$SalePrice,
ridge = ridge_train$SalePrice)
x_train <- log(x_train)
x_test <- data.frame(pls = pls_test$SalePrice[pls_test$Id %in% xgb_test$Id],
#rf = rf_test$SalePrice[rf_test$Id %in% xgb_test$Id],
xgb = xgb_test$SalePrice,
ridge = ridge_test$SalePrice)
x_test <- log(x_test)
df <- read.csv("featureMat_v2.csv", header = TRUE, stringsAsFactors = FALSE)
y_train = df$SalePrice[df$Train == 1]
#y_train <- exp(y_train)
glmnet_cv <- cv.glmnet(x=as.matrix(x_train), y=y_train,alpha=0)
lasso_fit <- glmnet(x=as.matrix(x_train), y=y_train, alpha=0, lambda = glmnet_cv$lambda.1se)
lasso_fit$beta
plot(predict(lasso_fit,as.matrix(x_train)), y_train)
abline(a=0,b=1,col="red")
y_pred <- data.frame(Id = 1461:2919,
SalePrice = exp(predict(lasso_fit, as.matrix(x_test))))
colnames(y_pred) <- c("Id", "SalePrice")
write.csv(y_pred, file="stacking_lasso.csv", row.names = FALSE)
rm(list = ls())
setwd("~/Documents/KaggleAmesHouse/xgboost_tune")
library(caret)
library(xgboost)
df <- read.csv("featureMat_v2.csv", header = TRUE, stringsAsFactors = FALSE)
x_train = df[df$Train == 1, 3:ncol(df)]
x_test = df[df$Train == 0, 3:ncol(df)]
y_train = df$SalePrice[df$Train == 1]
dtrain = xgb.DMatrix(as.matrix(x_train), label = y_train)
dtest = xgb.DMatrix(as.matrix(x_test))
xgb_params = list(
booster = 'gbtree',
#objective = 'reg:linear',
colsample_bytree=0.6,
eta=0.005,
max_depth=8,
min_child_weight=2,
#alpha=0.3,
#lambda=0.4,
gamma=0.01, # less overfit
subsample=0.5,
silent=TRUE)
bst = xgb.train(xgb_params,dtrain, nrounds = 10000)
y_pred.xgb = predict(bst, dtest)
y_train.xgb = predict(bst, dtrain)
y_pred <- data.frame(Id = 1461:2919 , SalePrice = exp(y_pred.xgb))
y_pred_train <- data.frame(Id = setdiff(1:1460, c(524,692,1183,1299)),
SalePrice = exp(y_train.xgb))
write.csv(y_pred, file="xgboost_test_fm2.csv", row.names = FALSE)
write.csv(y_pred_train, file="xgboost_train_fm2.csv", row.names = FALSE)
rm(list = ls())
setwd("~/Documents/KaggleAmesHouse/stacking")
library(glmnet)
pls_train <- read.csv("pls_train_fm2.csv", header = TRUE)
pls_test <- read.csv("pls_test_fm2.csv", header = TRUE)
rf_train <- read.csv("RFTrainPred.csv", header = TRUE)
rf_test <- read.csv("RFTestPred.csv", header = TRUE)
xgb_train <- read.csv("xgboost_train_fm2.csv", header = TRUE)
xgb_test <- read.csv("xgboost_test_fm2.csv", header = TRUE)
elastic <- read.csv("el-EMG-pred-CV-ALL.csv", header = TRUE)
elastic_train <- elastic[1:1460,]
elastic_test <- elastic[1461:2919,]
ridge_train <- read.csv("ridge_train_fm2.csv", header=TRUE, sep="\t")
ridge_test <- read.csv("ridge_test_fm2.csv", header=TRUE, sep="\t")
x_train <- data.frame(pls = pls_train$SalePrice[pls_train$Id %in% xgb_train$Id],
#rf = rf_train$SalePrice[rf_train$Id %in% xgb_train$Id],
xgb = xgb_train$SalePrice,
ridge = ridge_train$SalePrice)
#x_train <- log(x_train)
x_test <- data.frame(pls = pls_test$SalePrice[pls_test$Id %in% xgb_test$Id],
#rf = rf_test$SalePrice[rf_test$Id %in% xgb_test$Id],
xgb = xgb_test$SalePrice,
ridge = ridge_test$SalePrice)
#x_test <- log(x_test)
df <- read.csv("featureMat_v2.csv", header = TRUE, stringsAsFactors = FALSE)
y_train = df$SalePrice[df$Train == 1]
y_train <- exp(y_train)
glmnet_cv <- cv.glmnet(x=as.matrix(x_train), y=y_train,alpha=0)
lasso_fit <- glmnet(x=as.matrix(x_train), y=y_train, alpha=0, lambda = glmnet_cv$lambda.1se)
lasso_fit$beta
plot(predict(lasso_fit,as.matrix(x_train)), y_train)
abline(a=0,b=1,col="red")
y_pred <- data.frame(Id = 1461:2919,
SalePrice = predict(lasso_fit, as.matrix(x_test)))
colnames(y_pred) <- c("Id", "SalePrice")
write.csv(y_pred, file="stacking_lasso.csv", row.names = FALSE)
library(glmnet)
pls_train <- read.csv("pls_train_fm2.csv", header = TRUE)
pls_test <- read.csv("pls_test_fm2.csv", header = TRUE)
rf_train <- read.csv("RFTrainPred.csv", header = TRUE)
rf_test <- read.csv("RFTestPred.csv", header = TRUE)
xgb_train <- read.csv("xgboost_train_fm2.csv", header = TRUE)
xgb_test <- read.csv("xgboost_test_fm2.csv", header = TRUE)
elastic <- read.csv("el-EMG-pred-CV-ALL.csv", header = TRUE)
elastic_train <- elastic[1:1460,]
elastic_test <- elastic[1461:2919,]
ridge_train <- read.csv("ridge_train_fm2.csv", header=TRUE, sep="\t")
ridge_test <- read.csv("ridge_test_fm2.csv", header=TRUE, sep="\t")
x_train <- data.frame(pls = pls_train$SalePrice[pls_train$Id %in% xgb_train$Id],
#rf = rf_train$SalePrice[rf_train$Id %in% xgb_train$Id],
xgb = xgb_train$SalePrice,
ridge = ridge_train$SalePrice)
x_train <- log(x_train)
x_test <- data.frame(pls = pls_test$SalePrice[pls_test$Id %in% xgb_test$Id],
#rf = rf_test$SalePrice[rf_test$Id %in% xgb_test$Id],
xgb = xgb_test$SalePrice,
ridge = ridge_test$SalePrice)
x_test <- log(x_test)
df <- read.csv("featureMat_v2.csv", header = TRUE, stringsAsFactors = FALSE)
y_train = df$SalePrice[df$Train == 1]
#y_train <- exp(y_train)
glmnet_cv <- cv.glmnet(x=as.matrix(x_train), y=y_train,alpha=0)
lasso_fit <- glmnet(x=as.matrix(x_train), y=y_train, alpha=0, lambda = glmnet_cv$lambda.1se)
lasso_fit$beta
plot(predict(lasso_fit,as.matrix(x_train)), y_train)
abline(a=0,b=1,col="red")
glmnet_cv
x_train <- data.frame(pls = pls_train$SalePrice[pls_train$Id %in% xgb_train$Id],
#rf = rf_train$SalePrice[rf_train$Id %in% xgb_train$Id],
xgb = xgb_train$SalePrice,
ridge = ridge_train$SalePrice,
root_pls = sqrt(pls_train$SalePrice[pls_train$Id %in% xgb_train$Id]),
root_xgb = sqrt(xgb_train$SalePrice),
root_ridge = sqrt(ridge_train$SalePrice))
x_train <- log(x_train)
x_test <- data.frame(pls = pls_test$SalePrice[pls_test$Id %in% xgb_test$Id],
#rf = rf_test$SalePrice[rf_test$Id %in% xgb_test$Id],
xgb = xgb_test$SalePrice,
ridge = ridge_test$SalePrice,
root_pls = sqrt(pls_test$SalePrice[pls_test$Id %in% xgb_test$Id]),
root_xgb = sqrt(xgb_test$SalePrice),
root_ridge = sqrt(ridge_test$SalePrice))
x_test <- log(x_test)
df <- read.csv("featureMat_v2.csv", header = TRUE, stringsAsFactors = FALSE)
y_train = df$SalePrice[df$Train == 1]
#y_train <- exp(y_train)
glmnet_cv <- cv.glmnet(x=as.matrix(x_train), y=y_train,alpha=0)
lasso_fit <- glmnet(x=as.matrix(x_train), y=y_train, alpha=0, lambda = glmnet_cv$lambda.1se)
lasso_fit$beta
plot(predict(lasso_fit,as.matrix(x_train)), y_train)
abline(a=0,b=1,col="red")
y_pred <- data.frame(Id = 1461:2919,
SalePrice = exp(predict(lasso_fit, as.matrix(x_test))))
colnames(y_pred) <- c("Id", "SalePrice")
write.csv(y_pred, file="stacking_lasso.csv", row.names = FALSE)
rm(list = ls())
setwd("~/Documents/KaggleAmesHouse/stacking")
library(glmnet)
pls_train <- read.csv("pls_train_fm2.csv", header = TRUE)
pls_test <- read.csv("pls_test_fm2.csv", header = TRUE)
rf_train <- read.csv("RFTrainPred.csv", header = TRUE)
rf_test <- read.csv("RFTestPred.csv", header = TRUE)
xgb_train <- read.csv("xgboost_train_fm2.csv", header = TRUE)
xgb_test <- read.csv("xgboost_test_fm2.csv", header = TRUE)
elastic <- read.csv("el-EMG-pred-CV-ALL.csv", header = TRUE)
elastic_train <- elastic[1:1460,]
elastic_test <- elastic[1461:2919,]
ridge_train <- read.csv("ridge_train_fm2.csv", header=TRUE, sep="\t")
ridge_test <- read.csv("ridge_test_fm2.csv", header=TRUE, sep="\t")
x_train <- data.frame(pls = pls_train$SalePrice[pls_train$Id %in% xgb_train$Id],
#rf = rf_train$SalePrice[rf_train$Id %in% xgb_train$Id],
xgb = xgb_train$SalePrice,
ridge = ridge_train$SalePrice,
root_pls = sqrt(pls_train$SalePrice[pls_train$Id %in% xgb_train$Id]),
root_xgb = sqrt(xgb_train$SalePrice),
root_ridge = sqrt(ridge_train$SalePrice))
x_train <- log(x_train)
x_test <- data.frame(pls = pls_test$SalePrice[pls_test$Id %in% xgb_test$Id],
#rf = rf_test$SalePrice[rf_test$Id %in% xgb_test$Id],
xgb = xgb_test$SalePrice,
ridge = ridge_test$SalePrice,
root_pls = sqrt(pls_test$SalePrice[pls_test$Id %in% xgb_test$Id]),
root_xgb = sqrt(xgb_test$SalePrice),
root_ridge = sqrt(ridge_test$SalePrice))
x_test <- log(x_test)
df <- read.csv("featureMat_v2.csv", header = TRUE, stringsAsFactors = FALSE)
y_train = df$SalePrice[df$Train == 1]
#y_train <- exp(y_train)
glmnet_cv <- cv.glmnet(x=as.matrix(x_train), y=y_train,alpha=0)
lasso_fit <- glmnet(x=as.matrix(x_train), y=y_train, alpha=0, lambda = glmnet_cv$lambda.1se)
lasso_fit$beta
plot(predict(lasso_fit,as.matrix(x_train)), y_train)
abline(a=0,b=1,col="red")
y_pred <- data.frame(Id = 1461:2919,
SalePrice = exp(predict(lasso_fit, as.matrix(x_test))))
colnames(y_pred) <- c("Id", "SalePrice")
write.csv(y_pred, file="stacking_lasso.csv", row.names = FALSE)
lasso_fit$a0
?glmnet
